apiVersion: camel.apache.org/v1
kind: Integration
metadata:
  name: kafka-to-s3-integration
spec:
  configuration:
    - type: configmap
      value: kafka-to-s3-config
    - type: secret
      value: odf-secret
  profile: OpenShift
  sources:
    - content: |
        package com.redhat.manuela.routes;
        import java.io.ByteArrayInputStream;
        import java.util.Iterator;
        import java.util.List;

        import org.apache.camel.Exchange;
        import org.apache.camel.Processor;
        import org.apache.camel.PropertyInject;
        import org.apache.camel.builder.RouteBuilder;
        import org.apache.camel.component.aws2.s3.AWS2S3Constants;
        import org.apache.camel.builder.endpoint.dsl.AWS2S3EndpointBuilderFactory;
        import org.apache.camel.model.OnCompletionDefinition;
        import org.apache.camel.processor.aggregate.GroupedBodyAggregationStrategy;
        import org.slf4j.Logger;
        import org.slf4j.LoggerFactory;

        public class Kafka2S3Route extends RouteBuilder {

            private static final Logger LOGGER = LoggerFactory.getLogger(Kafka2S3Route.class);

            @PropertyInject("s3.custom.endpoint.enabled")
            private String s3_custom_endpoint_enabled;

            @PropertyInject("s3.custom.endpoint.url")
            private String s3_custom_endpoint_url;

            @PropertyInject("s3.accessKey")
            private String s3_accessKey;
            @PropertyInject("s3.secretKey")
            private String s3_secretKey;
            @PropertyInject("s3.message.aggregation.count")
            private String s3_message_aggregation_count;

            @PropertyInject("s3.region")
            private String s3_region;
            @Override
            public void configure() throws Exception {
                storeInS3();
            }

            private void storeInS3() {
                String key = "accessKey=RAW(" + s3_accessKey + ")";
                String secret = "&secretKey=RAW(" + s3_secretKey + ")";
                String region = "&region=" + s3_region;
                String override_endpoint = "&overrideEndpoint=true";
                String uriendpoint_override = "&uriEndpointOverride=" + s3_custom_endpoint_url;
                String s3params = key
                   + secret
                   + region
                   + override_endpoint
                   + uriendpoint_override;
                from("kafka:{{ .Values.kafka.broker.topic }}?brokers={{ .Values.kafka.broker.uri }}")
                    .convertBodyTo(String.class)
                    .aggregate(simple("true"), new GroupedBodyAggregationStrategy()).completionSize(s3_message_aggregation_count)
                    .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                List<Exchange> data = exchange.getIn().getBody(List.class);
                                StringBuffer sb = new StringBuffer();
                                for (Iterator iterator = data.iterator(); iterator.hasNext();) {
                                    String ex = (String) iterator.next();
                                    sb.append(ex+"\n");
                                }
                                exchange.getIn().setBody(new ByteArrayInputStream(sb.toString().getBytes()));
                            }
                        })
                    // .to(\"file:/var/tmp/\");
                    .setHeader(AWS2S3Constants.KEY, simple("dgw-${headers[kafka.KEY]}-${date:now}.txt"))
                    .to("aws2-s3://{{ .Values.global.s3.bucket.name }}?" + s3params)
                    .log("Uploaded Temperature from [ ${headers[kafka.KEY]} ] dataset to S3");
            }
            @Override
            public OnCompletionDefinition onCompletion() {
                return super.onCompletion();
            }
        }
      name: Kafka2S3Route.java